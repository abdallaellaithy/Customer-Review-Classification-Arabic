{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abdal\n",
      "[nltk_data]     Maged\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abdal\n",
      "[nltk_data]     Maged\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Abdal\n",
      "[nltk_data]     Maged\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>مثير للدهشة حتى بالنسبة لغير اللاعب: كان هذا ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>أفضل مقطع صوتي على الإطلاق لأي شيء: أقرأ الكثي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>مذهل!: هذه الموسيقى التصويرية هي موسيقاي المفض...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>الموسيقى التصويرية الممتازة: أحب هذا الموسيقى ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>تذكر ، اسحب فكك عن الأرض بعد سماعها: إذا كنت ق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114995</th>\n",
       "      <td>0</td>\n",
       "      <td>DOA: فتح العلامة التجارية الجديدة من Box.تم تث...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114996</th>\n",
       "      <td>0</td>\n",
       "      <td>شركة صعبة التعامل معها: المنتج كان على ما يرام...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114997</th>\n",
       "      <td>0</td>\n",
       "      <td>SDK Sansa Leather Case: فقير للغاية.لم يتم الإ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114998</th>\n",
       "      <td>0</td>\n",
       "      <td>حسنًا ، لكن ليس رائعًا: حسنًا ، لقد اشتريت هذا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114999</th>\n",
       "      <td>1</td>\n",
       "      <td>مريحة جدا!: هذه النعال رائعة!أنها ناعمة جدا وم...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                            content\n",
       "0           1  مثير للدهشة حتى بالنسبة لغير اللاعب: كان هذا ا...\n",
       "1           1  أفضل مقطع صوتي على الإطلاق لأي شيء: أقرأ الكثي...\n",
       "2           1  مذهل!: هذه الموسيقى التصويرية هي موسيقاي المفض...\n",
       "3           1  الموسيقى التصويرية الممتازة: أحب هذا الموسيقى ...\n",
       "4           1  تذكر ، اسحب فكك عن الأرض بعد سماعها: إذا كنت ق...\n",
       "...       ...                                                ...\n",
       "114995      0  DOA: فتح العلامة التجارية الجديدة من Box.تم تث...\n",
       "114996      0  شركة صعبة التعامل معها: المنتج كان على ما يرام...\n",
       "114997      0  SDK Sansa Leather Case: فقير للغاية.لم يتم الإ...\n",
       "114998      0  حسنًا ، لكن ليس رائعًا: حسنًا ، لقد اشتريت هذا...\n",
       "114999      1  مريحة جدا!: هذه النعال رائعة!أنها ناعمة جدا وم...\n",
       "\n",
       "[115000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing Stopwords\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Removing Punctuation and Special Characters\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "\n",
    "    # Stemming or Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Filter only Arabic words using regex\n",
    "    arabic_words = [word for word in lemmatized_tokens if re.fullmatch('[\\u0600-\\u06FF]+', word)]\n",
    "    \n",
    "    return arabic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applying Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprossing'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مثير', 'للدهشة', 'بالنسبة', 'لغير', 'اللاعب', 'المسار', 'الصوتي', 'يرسم', 'أعضاء', 'مجلس', 'الشيوخ', 'عقلك', 'وأوصي', 'للأشخاص', 'يكرهون', 'اللعبة', 'لقد', 'لعبت', 'لعبة', 'الألعاب', 'لعبتها', 'الإطلاق', 'لديها', 'أفضل', 'موسيقى', 'يعود', 'لوحة', 'المفاتيح', 'الخام', 'ويأخذ', 'خطوة', 'طازجة', 'القيثارات', 'الصار', 'والأوركسترا', 'شأنه', 'يثير', 'إعجاب', 'شخص', 'يهتم', 'بالاستماع']\n",
      "41\n",
      "مثير للدهشة حتى بالنسبة لغير اللاعب: كان هذا المسار الصوتي جميلًا!إنه يرسم أعضاء مجلس الشيوخ في عقلك جيدًا ، وأوصي حتى للأشخاص الذين يكرهون فيد.موسيقى اللعبة!لقد لعبت لعبة Chrono Cross ولكن من بين جميع الألعاب التي لعبتها على الإطلاق لديها أفضل موسيقى!إنه يعود بعيدًا عن لوحة المفاتيح الخام ويأخذ خطوة طازجة مع القيثارات الصار والأوركسترا العاطفية.من شأنه أن يثير إعجاب أي شخص يهتم بالاستماع!^_^\n",
      "395\n"
     ]
    }
   ],
   "source": [
    "print(df['preprossing'].iloc[0])\n",
    "print(len(df['preprossing'].iloc[0]))\n",
    "\n",
    "print(df['content'].iloc[0])\n",
    "print(len(df['content'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word Embedding</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embbeding(sentences):\n",
    "    # Filter out empty sentences\n",
    "    sentences = [sentence for sentence in sentences if sentence]\n",
    "\n",
    "    # Word2Vec model training\n",
    "    w2v_model = Word2Vec(sentences, min_count=1, vector_size=100)  # Adjust vector_size as needed\n",
    "    return w2v_model\n",
    "\n",
    "# Example usage\n",
    "sentences = list(df['preprossing'])\n",
    "w2v_model = embbeding(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Spliting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['preprossing'])\n",
    "y = np.array(df['label'])\n",
    "X\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Define the Word2Vec model\n",
    "model = Word2Vec(sentences=X, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model.train(X, total_examples=len(X), epochs=10)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data_shape:  (103500,)\n",
      "Test_data_shape:  (11500,)\n"
     ]
    }
   ],
   "source": [
    "print('Train_data_shape: ', X_train.shape)\n",
    "print('Test_data_shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec_model.model\")\n",
    "\n",
    "# Function to convert a list of words into a fixed-size vector\n",
    "def sentence_to_vector(sentence, model):\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert X_train and X_test to vectors\n",
    "X_train_vectors = [sentence_to_vector(sentence, model) for sentence in X_train]\n",
    "X_test_vectors = [sentence_to_vector(sentence, model) for sentence in X_test]\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "X_train_vectors = np.array(X_train_vectors)\n",
    "X_test_vectors = np.array(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "word_size = 100\n",
    "# Create a simple feedforward neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(word_size,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3235/3235 [==============================] - 5s 1ms/step - loss: 0.3855 - accuracy: 0.8272 - val_loss: 0.3788 - val_accuracy: 0.8323\n",
      "Epoch 2/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3632 - accuracy: 0.8388 - val_loss: 0.3726 - val_accuracy: 0.8349\n",
      "Epoch 3/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3540 - accuracy: 0.8428 - val_loss: 0.3811 - val_accuracy: 0.8315\n",
      "Epoch 4/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3458 - accuracy: 0.8469 - val_loss: 0.3689 - val_accuracy: 0.8370\n",
      "Epoch 5/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3388 - accuracy: 0.8502 - val_loss: 0.3708 - val_accuracy: 0.8363\n",
      "Epoch 6/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3322 - accuracy: 0.8523 - val_loss: 0.3692 - val_accuracy: 0.8384\n",
      "Epoch 7/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3255 - accuracy: 0.8558 - val_loss: 0.3740 - val_accuracy: 0.8364\n",
      "Epoch 8/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3189 - accuracy: 0.8594 - val_loss: 0.3790 - val_accuracy: 0.8330\n",
      "Epoch 9/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3128 - accuracy: 0.8630 - val_loss: 0.3852 - val_accuracy: 0.8355\n",
      "Epoch 10/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3073 - accuracy: 0.8653 - val_loss: 0.3848 - val_accuracy: 0.8346\n",
      "Epoch 11/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.3020 - accuracy: 0.8675 - val_loss: 0.3929 - val_accuracy: 0.8297\n",
      "Epoch 12/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2959 - accuracy: 0.8709 - val_loss: 0.4090 - val_accuracy: 0.8310\n",
      "Epoch 13/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2906 - accuracy: 0.8735 - val_loss: 0.3997 - val_accuracy: 0.8355\n",
      "Epoch 14/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2847 - accuracy: 0.8762 - val_loss: 0.4001 - val_accuracy: 0.8320\n",
      "Epoch 15/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2797 - accuracy: 0.8777 - val_loss: 0.4078 - val_accuracy: 0.8317\n",
      "Epoch 16/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2740 - accuracy: 0.8815 - val_loss: 0.4150 - val_accuracy: 0.8315\n",
      "Epoch 17/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2699 - accuracy: 0.8843 - val_loss: 0.4216 - val_accuracy: 0.8297\n",
      "Epoch 18/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2645 - accuracy: 0.8862 - val_loss: 0.4390 - val_accuracy: 0.8263\n",
      "Epoch 19/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2601 - accuracy: 0.8886 - val_loss: 0.4446 - val_accuracy: 0.8290\n",
      "Epoch 20/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2554 - accuracy: 0.8904 - val_loss: 0.4492 - val_accuracy: 0.8267\n",
      "Epoch 21/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2502 - accuracy: 0.8933 - val_loss: 0.4561 - val_accuracy: 0.8251\n",
      "Epoch 22/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2463 - accuracy: 0.8948 - val_loss: 0.4630 - val_accuracy: 0.8232\n",
      "Epoch 23/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2421 - accuracy: 0.8965 - val_loss: 0.4685 - val_accuracy: 0.8195\n",
      "Epoch 24/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2388 - accuracy: 0.8976 - val_loss: 0.4794 - val_accuracy: 0.8180\n",
      "Epoch 25/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2345 - accuracy: 0.8994 - val_loss: 0.4938 - val_accuracy: 0.8220\n",
      "Epoch 26/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2313 - accuracy: 0.9019 - val_loss: 0.4941 - val_accuracy: 0.8216\n",
      "Epoch 27/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2276 - accuracy: 0.9030 - val_loss: 0.5105 - val_accuracy: 0.8131\n",
      "Epoch 28/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2239 - accuracy: 0.9048 - val_loss: 0.5110 - val_accuracy: 0.8147\n",
      "Epoch 29/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2203 - accuracy: 0.9066 - val_loss: 0.5403 - val_accuracy: 0.8130\n",
      "Epoch 30/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2173 - accuracy: 0.9075 - val_loss: 0.5425 - val_accuracy: 0.8130\n",
      "Epoch 31/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2139 - accuracy: 0.9096 - val_loss: 0.5491 - val_accuracy: 0.8183\n",
      "Epoch 32/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2101 - accuracy: 0.9108 - val_loss: 0.5650 - val_accuracy: 0.8135\n",
      "Epoch 33/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2086 - accuracy: 0.9121 - val_loss: 0.5668 - val_accuracy: 0.8116\n",
      "Epoch 34/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2052 - accuracy: 0.9129 - val_loss: 0.5712 - val_accuracy: 0.8123\n",
      "Epoch 35/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2023 - accuracy: 0.9147 - val_loss: 0.5864 - val_accuracy: 0.8085\n",
      "Epoch 36/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.2002 - accuracy: 0.9155 - val_loss: 0.5966 - val_accuracy: 0.8111\n",
      "Epoch 37/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.1966 - accuracy: 0.9175 - val_loss: 0.6090 - val_accuracy: 0.8065\n",
      "Epoch 38/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.1955 - accuracy: 0.9183 - val_loss: 0.6172 - val_accuracy: 0.8079\n",
      "Epoch 39/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.1916 - accuracy: 0.9194 - val_loss: 0.6353 - val_accuracy: 0.8040\n",
      "Epoch 40/40\n",
      "3235/3235 [==============================] - 4s 1ms/step - loss: 0.1900 - accuracy: 0.9202 - val_loss: 0.6360 - val_accuracy: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22200301e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using your data\n",
    "model.fit(X_train_vectors, y_train, epochs=40, batch_size=32, validation_data=(X_test_vectors, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 717us/step\n",
      "Accuracy: 0.808\n",
      "Confusion Matrix:\n",
      "[[4483 1138]\n",
      " [1070 4809]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      5621\n",
      "           1       0.81      0.82      0.81      5879\n",
      "\n",
      "    accuracy                           0.81     11500\n",
      "   macro avg       0.81      0.81      0.81     11500\n",
      "weighted avg       0.81      0.81      0.81     11500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_vectors)\n",
    "\n",
    "# Convert predicted probabilities to binary labels (0 or 1)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate the model performance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "classification_report_str = classification_report(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted Sentiment: 1\n",
      "Predicted Sentiment: [[0.9999141]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model.model\")\n",
    "\n",
    "# Load the pre-trained neural network model\n",
    "# nn_model = load_model(r\"I:\\University\\$$$$Forth_Year$$$$\\semester_1\\NLP\\Project\\your_model_name.h5\")  # Replace with the actual path to your saved model\n",
    "\n",
    "def sentence_to_vector(sentence, model):\n",
    "    word_vectors = [model.wv[word] for word in sentence.split() if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def predict_sentiment(user_input, word2vec_model, nn_model):\n",
    "    # Convert user input to Word2Vec vectors\n",
    "    user_input_vector = sentence_to_vector(user_input, word2vec_model)\n",
    "\n",
    "    # Reshape the vector to match the input shape of the neural network\n",
    "    user_input_vector = user_input_vector.reshape(1, -1)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = nn_model.predict(user_input_vector)\n",
    "\n",
    "    # Convert predicted probability to a binary label\n",
    "    predicted_label = 1 if prediction > 0.5 else 0\n",
    "\n",
    "    return predicted_label, prediction\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "predicted_sentiment,prediction = predict_sentiment(user_input, word2vec_model, model)\n",
    "\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "print(f\"Predicted Sentiment: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# model.save(r'I:\\University\\$$$$Forth_Year$$$$\\semester_1\\NLP\\Project\\your_model_name.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# model = load_model(r'I:\\University\\$$$$Forth_Year$$$$\\semester_1\\NLP\\Project\\your_model_name.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
